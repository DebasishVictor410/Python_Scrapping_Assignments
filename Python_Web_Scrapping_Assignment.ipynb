{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61510d58",
   "metadata": {},
   "source": [
    "### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb6b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### API\n",
    "#### The process of extracting data from a webpage is known as web scraping.\n",
    "#### This data is gathered and then exported \n",
    "#### It is done in a way that the user will find more value from that\n",
    "#### it canbe stored in a spreadsheet or an API\n",
    "#### we can also use database like sql,nosql for that\n",
    "#### web scrapping can be done with the help of API\n",
    "#### as we are using python so we can use flask , Django as an API\n",
    "#### with the help of perferred api and python we can scarp data\n",
    "\n",
    "#### Why it is used?\n",
    "#### many real estate agents employ web scraping.\n",
    "#### To use that for sale or rent benefit purpose \n",
    "#### For instance, a real estate company might construct an API \n",
    "#### from a website that automatically adds this data to their website. \n",
    "#### many companies use scrapping for building huge database\n",
    "#### they use that database to take insights of that\n",
    "#### use that knowledge to get more profit\n",
    "#### Several websites and programs can assist you in \n",
    "#### effortlessly comparing prices between multiple sellers \n",
    "#### for the same product.\n",
    "#### in this issue web scrapping can help\n",
    "#### These websites function in part by employing web scrapers \n",
    "#### to collect product data and pricing from each merchant \n",
    "#### they do it on a daily basis. \n",
    "#### This allows them to give their users with the necessary comparison data\n",
    "#### Many businesses utilize web scraping to obtain contact information \n",
    "#### from potential consumers or clients. \n",
    "#### This is extremely frequent in the business-to-business market\n",
    "#### as potential clients would publicly display their company details \n",
    "#### they do it online.\n",
    "\n",
    "#### Areas\n",
    "#### 1. Marketing Lead Generation\n",
    "#### Web scraping software can be used to create marketing leads. \n",
    "#### Scraping data from relevant websites can be used to create email \n",
    "#### and phone lists for cold outreach. \n",
    "#### Business contact information such as phone numbers and email addresses\n",
    "#### can be scrapped from that\n",
    "#### for example, data can be scraped from yellow pages websites \n",
    "#### or mapping related web sites\n",
    "#### 2. Electronic commerce\n",
    "#### Web scraping can be used to harvest product data from \n",
    "#### various e-commerce websites such as Amazon, eBay, Google Shopping\n",
    "#### and so on on a regular basis. \n",
    "#### Product information such as price, description, photos, reviews\n",
    "#### ratings, and so on can be simply collected utilizing web scraping\n",
    "#### 3. Data Examination\n",
    "#### You could want to collect and evaluate data from numerous websites \n",
    "#### about a given category. \n",
    "#### Real estate, autos, electrical gadgets, industrial equipment\n",
    "#### business relationships, marketing, and so forth. \n",
    "#### The various websites that belong to the specific category display \n",
    "#### information in various formats. \n",
    "#### Even if you only have one website, you may not be able to \n",
    "#### see all of the data at once. \n",
    "#### The data may be spread across numerous pages \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae72138",
   "metadata": {},
   "source": [
    "### Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2341999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Human Copying and Pasting\n",
    "#### The most basic type of web scraping is manually copying and pasting \n",
    "#### data from a web page into a text file or spreadsheet. \n",
    "#### Even the most advanced web scraping technology cannot always replace \n",
    "#### a human's manual analysis and copy-and-paste\n",
    "#### and this may be the only acceptable alternative \n",
    "#### when scraping websites explicitly ban machine automation.\n",
    "#### Text Pattern Recognition\n",
    "#### The UNIX grep command or programming language regular expression\n",
    "#### matching features can be used to extract information from web pages \n",
    "#### in a simple yet effective manner.\n",
    "#### HTTP Development\n",
    "#### Static and dynamic web pages can be obtained by sending HTTP requests\n",
    "#### to a remote web server via socket programming.\n",
    "#### Parsing HTML\n",
    "#### Many websites incorporate enormous collections of dynamically created\n",
    "#### pages from an underlying structured source, such as a database. \n",
    "#### Typically, a shared script or template is used to encode data \n",
    "#### from the same category into similar pages. \n",
    "#### A wrapper is a data mining tool that recognizes \n",
    "#### such templates in a specific data source, extracts its content\n",
    "#### and converts it to a relational format.\n",
    "#### Parsing DOM\n",
    "#### It means Document Object Model\n",
    "#### By embedding a full-fledged web browser, \n",
    "#### such as Internet Explorer or the Mozilla browser control, \n",
    "#### programs can obtain dynamic material generated by client-side scripts.\n",
    "#### These browser features also parse web pages into a DOM tree, \n",
    "#### which can be used by programs to get sections of the pages. \n",
    "#### Languages such as Xpath can be used to parse the generated DOM tree.\n",
    "#### Vertical Grouping\n",
    "#### Several businesses have developed vertically focused harvesting \n",
    "#### platforms. These platforms create and manage a slew of \n",
    "#### \"bots\" for various verticals with no \"man in the loop\" \n",
    "#### no work tied to a specific target site. \n",
    "#### The preparation includes developing a knowledge base \n",
    "#### for the entire vertical, \n",
    "#### following which the platform will automatically create the bots.\n",
    "#### Semantic Annotation Recognizing\n",
    "#### The scraped pages may include metadata, semantic markups\n",
    "#### and annotations that can be used to locate specific data snippets. \n",
    "#### This technique can be viewed as a subset of DOM parsing \n",
    "#### if the annotations are embedded in the pages, as Microformat does. \n",
    "#### In another case, the annotations are stored and managed separately \n",
    "#### from the web pages, so scrapers can retrieve data schema \n",
    "#### and instructions from this layer before scraping the pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ae914",
   "metadata": {},
   "source": [
    "### Q3. What is Beautiful Soup? Why is it used?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b54fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Beautiful Soup\n",
    "#### Beautiful Soup is a Python package for easily scraping information \n",
    "#### from web pages. \n",
    "#### It sits on top of an HTML or XML parser \n",
    "#### and provides Pythonic idioms for iterating through, searching through\n",
    "#### and altering the parse tree.\n",
    "#### Beautiful Soup includes a few straightforward methods\n",
    "#### and Pythonic idioms for exploring, searching, and altering a parse tree\n",
    "#### Beautiful Soup translates incoming documents to Unicode \n",
    "#### and outgoing documents to UTF-8 automatically.\n",
    "#### Beautiful Soup is built on top of popular Python parsers \n",
    "#### such as lxml and html5lib, allowing us to experiment with \n",
    "#### alternative parsing algorithms and exchange speed for flexibility.\n",
    "\n",
    "#### Why?\n",
    "#### Beautiful Soup is a Python library that is used for web scraping.\n",
    "#### It allows you to parse HTML and XML documents and extract \n",
    "#### the data you need. It is commonly used for data mining\n",
    "#### and data analysis, as well as automating tasks such \n",
    "#### as filling out forms or logging into websites.\n",
    "#### It is commonly used in conjunction with urllib or the requests \n",
    "#### module in Python to extract required information from \n",
    "#### a website represented by its url, also known as web scraping.\n",
    "#### thus Beautiful Soup works \n",
    "#### it scrapes data from websites.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba8860",
   "metadata": {},
   "source": [
    "### Q4. Why is flask used in this Web Scraping project?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42abec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Flask is a web development framework that is lightweight. \n",
    "#### This will be used to parse our obtained data \n",
    "#### and present it as HTML in a new HTML file. \n",
    "#### The requests module allows us to submit HTTP queries \n",
    "#### to the website we want to scrape. \n",
    "#### The first line imports the Flask class \n",
    "#### and the render_template method from the flask library.\n",
    "#### unlike the opinionated Django framework, is more adaptable \n",
    "#### to varied working styles and approaches to web app development. \n",
    "#### Flask is favoured by programmers who have more coding experience \n",
    "#### or need more flexibility over the project design.\n",
    "#### Because there is no default model, Flask supports different types \n",
    "#### of databases by default. \n",
    "#### This also makes database integration into Flask apps easier.\n",
    "#### If you want to build a small web app with a few static pages, \n",
    "#### Flask will make your life easier than Django. \n",
    "#### For smaller web applications, many programmers find Flask to be easily \n",
    "#### scalable as It includes a development server and a quick debugger.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdfe5e4",
   "metadata": {},
   "source": [
    "### Q5. Write the names of AWS services used in this project. Also, explain the use of each service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8eb0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Amazon Web Services (AWS) is the world's largest cloud computing \n",
    "#### platform, with over 200 universally available services \n",
    "#### ranging from infrastructure to machine intelligence. \n",
    "#### These combinable solutions enable optimum usability and \n",
    "#### are specifically designed to optimize the speed of your application \n",
    "#### through content delivery features, data storage, and more.\n",
    "\n",
    "#### Services used in this project\n",
    "#### CodePipeline\n",
    "#### AWS CodePipeline is a continuous delivery solution that allows you to model, \n",
    "#### visualize, and automate the steps required for software deployment. \n",
    "#### The various stages of a software release process can be simply modelled and configured. \n",
    "#### CodePipeline streamlines the actions required to continually release software modifications.\n",
    "\n",
    "#### Beanstalk\n",
    "#### AWS An AWS-managed service for web applications is called Elastic Beanstalk.\n",
    "#### A pre-configured EC2 server called Elastic Beanstalk can directly accept your application code \n",
    "#### it creates environment configurations and utilize them to automatically provision \n",
    "#### and deploy the resources needed in AWS to operate the web application. \n",
    "#### Elastic Beanstalk, in contrast to EC2, is a Platform As A Service (PAAS)\n",
    "#### It allowing users to directly employ a pre-configured server for their application. \n",
    "\n",
    "#### Some Other Service\n",
    "#### 1. Amazon Elastic Compute Cloud (EC2)\n",
    "#### Amazon EC2 is a cloud platform that provides secure \n",
    "#### and scalable compute capability. \n",
    "#### goal is to provide developers with easy access and usability \n",
    "#### for web-scale cloud computing while giving you complete \n",
    "#### control over your compute resources.\n",
    "#### 2. RDS (Relational Database Services) from Amazon\n",
    "#### Amazon RDS (Amazon Relational Database Service) simplifies database \n",
    "#### configuration, management, and growth in the cloud. Automate time-consuming \n",
    "#### operations including hardware provisioning, database configuration, patching, \n",
    "#### and backups in a cost-effective and appropriate manner.\n",
    "#### Amazon Connect \n",
    "#### Amazon Connect is a cloud-based contact center that is simple to set up and deploy. \n",
    "#### Take use of industry-leading features including telephony, chat, routing, task management, and much more.\n",
    "#### 3. Amazon S3 (Simple Storage Service)\n",
    "#### Amazon S3, at its core, facilitates object storage, providing leading scalability, data availability, \n",
    "#### security, and performance. Businesses of vast sizes can leverage S3 for storage and protect \n",
    "#### large sums of data for various use cases, such as websites, applications, backup, and more.\n",
    "#### Amazon S3’s intuitive management features enable the frictionless organization of data and \n",
    "#### configurable access controls.\n",
    "#### Lambda from Amazon\n",
    "#### Lambda allows you to run code without having to own or manage servers. \n",
    "#### Users are only charged for the compute time used.\n",
    "#### Without administration, run code for almost any application or backend utility. \n",
    "#### Users simply upload the code, and Lambda handles the rest, resulting in exact software scaling and high availability.\n",
    "#### Amazon Cognito is a fifth option.\n",
    "#### AWS Cognito manages a control access dashboard for onboarding users to their online and mobile\n",
    "#### apps via sign-up and sign-in capabilities. AWS Cognito grows to millions of \n",
    "#### people and supports sign-in via SAML 2.0 with social identity providers such as Facebook, \n",
    "#### Google, and Amazon, as well as enterprise identity providers.\n",
    "#### Amazon Glacier\n",
    "#### AWS Glacier services are Amazon S3 cloud storage classes that are secure, configurable, \n",
    "#### and cost-effective for data caching and long-term backup. These storage classes \n",
    "#### enable reliable delivery while also providing extensive security and compliance capabilities and \n",
    "#### meeting regulatory requirements.Users can store data for as little as $1 per terabyte per month, \n",
    "#### saving them money both up front and in the long run when compared to on-premises servers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ea079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e561ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
